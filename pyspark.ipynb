{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gagankpatra/data-science-notebooks/blob/master/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_Y10Ogp9ANz8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Upload files (not folder)<30MB from local machine\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload() # then browse, select the files. It's then uploaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7vk7rAI7U0B-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bP9QMxKTF7UR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install p7zip-full\n",
        "!apt-get install vim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cBrGqxqVGQ9O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!p7zip -d vi.stackexchange.com.7z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x1X4D8PwGczo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir stackexchange\n",
        "!mv *.xml stackexchange\n",
        "!ls stackexchange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHRaek-OG3J4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls stackexchange/Posts.xml\n",
        "!cat stackexchange/Posts.xml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o2pnLb7NI6te",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Map Filter and Reduce (Python) - Functions that facilitate a functional programming approach\n",
        "\n",
        "* Map - apply a function to all items on a list\n",
        "* Filter - create new list for items that meet certain criteria\n",
        "* Reduce - perform a computation on a list"
      ]
    },
    {
      "metadata": {
        "id": "-XdVFiHVIYFq",
        "colab_type": "code",
        "outputId": "38333cd1-7127-4a6e-9a7a-42ec406ef69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "numbers_list = [1,2,3,4,5]\n",
        "\n",
        "def  add_one (my_item):\n",
        "  return my_item + 1\n",
        "\n",
        "list(map(add_one, numbers_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 5, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "aHnzdRN_JtfV",
        "colab_type": "code",
        "outputId": "bbe0c52f-6965-455b-e57a-adbf919ec61c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def  is_even(my_item):\n",
        "  return my_item%2 == 0\n",
        "\n",
        "\n",
        "list(filter(is_even, numbers_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "TJJnY54BKzcS",
        "colab_type": "code",
        "outputId": "8180a21b-fb62-4adc-d681-a0017ea575f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def add_items(first,second):\n",
        "  return first+second\n",
        "\n",
        "import functools #python3\n",
        "functools.reduce (add_items, numbers_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "cWA1xHgvMKgF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gIYN8rB0b6xj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7J5PcgnudYub",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "azilePL3nVu5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SparkConf with RDD API\n",
        "\n",
        "from pyspark import SparkConf,SparkContext\n",
        "\n",
        "#sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "conf=(SparkConf()\n",
        "     .setMaster(\"local[*]\")\n",
        "     .setAppName(\"Stack Overflow Test\")\n",
        "     .set(\"spark.executor.memory\",\"2g\"))\n",
        "\n",
        "sc = SparkContext(conf=conf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q2d5oSQFCcBT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SparkConf with RDD API\n",
        "\n",
        "from pyspark import SparkConf,SparkContext\n",
        "\n",
        "#sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "conf=(SparkConf()\n",
        "     .setMaster(\"yarn\")\n",
        "     .setAppName(\"Stack Overflow Test\")\n",
        "     .set(\"spark.executor.memory\",\"2g\"))\n",
        "\n",
        "sc = SparkContext(conf=conf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "axhh_mjMKwFD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Review configuration\n",
        "sc.getConf().getAll()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bo5xtNzaLy9q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Specific configuration\n",
        "sc.getConf().get()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JpLjEj0KI1mh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SparkConf with RDD API (Another way by using setSystemProperty)\n",
        "\n",
        "from pyspark import SparkContext\n",
        "SparkContext.setSystemProperty('spark.executor.memory','2g')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BSOAFLv4IsVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# You can include file dependencies with pyFiles\n",
        "\n",
        "sc= SparkContext(\"yarn\",\"StackOverflowTest\",py-Files=['sotest.py','lib.zip'])\n",
        "\n",
        "#SparkConf with spark2-submit\n",
        "\n",
        "spark2-submit --executor-memory 4g stackoverflowtest.py    #Also possible to use --config \"spark.executor.memory=4g\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HbjZf2EfMH_k",
        "colab_type": "code",
        "outputId": "21a040d2-8642-4504-cb0b-fd373801a88e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#SparkConf with DataFrame API\n",
        "\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "SparkSession.builder.config(conf=SparkConf())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession.Builder at 0x7fbac3859278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "7ur7FUstaTqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#builder Pattern - Complex object ,one step at a time\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "               .master(\"local[*]\") \\\n",
        "               .appName('StackOverflowTest') \\\n",
        "               .config('spark.executor.memory','2g') \\\n",
        "               .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rqF-7fefNEkQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Another useful configuration is the \"deployment mode\" that specifies location of where the Spark driver executes\n",
        "\n",
        "# Client Deployment Mode \n",
        "# Driver runs where Spark application was launched i.e local machine.\n",
        "# Process killed if driver disconnected\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "               .master(\"local[*]\") \\\n",
        "               .appName('StackOverflowTest') \\\n",
        "               .config('spark.submit.deployMode','client') \\\n",
        "               .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FOvzhskrOUDk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cluster Deployment Mode\n",
        "# Driver runs in a node within the cluster even if launched from outside\n",
        "# Process is not killed if computer where submitted is disconnected\n",
        "# Doesnot support shell\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "               .master(\"local[*]\") \\\n",
        "               .appName('StackOverflowTest') \\\n",
        "               .config('spark.submit.deployMode','cluster') \\\n",
        "               .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0lF1nnWHONyg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFpzJo6Peei_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir spark_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1loBg9AqmBdZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv \"spark commitors\".tsv spark_data\n",
        "!mv spark_data/\"spark commitors\".tsv spark_data/spark_commitors.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E5M5gaFSmlfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv spark_commitors_wo_header.tsv spark_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3gIU3x0eHZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls spark_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mNbhxrZjnbx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat spark_data/spark_commitors.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8fFsmS5gkdsm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!sed 1d spark_data/spark_commitors.tsv > spark_data/spark_commitors_wo_header.tsv\n",
        "!cat spark_data/spark_commitors_wo_header.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YvFv2mg9nAE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('spark_data/spark_commitors..tsv') #upload to s3\n",
        "files.download('spark_data/spark_commitors_wo_header.tsv') #upload to s3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k-NNH6_4pD6q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SparkContext**\n",
        "* Located in the spark driver\n",
        "* Main entry point for Spark functionality. \n",
        "* A SparkContext represents the connection to a Spark cluster, and can be used to create RDD and broadcast variables on that cluster.\n",
        "* The Spark application\n",
        " * One SparkContext per application\n",
        "* Created for you in REPL\n",
        "* You need to create for spark2-submit\n"
      ]
    },
    {
      "metadata": {
        "id": "utbBz7jQLr6z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sc\n",
        "sc.version\n",
        "type(sc)\n",
        "help(sc)\n",
        "dir(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uPfFOxgmMiS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sc.appName\n",
        "sc.uiWebUrl\n",
        "sc.applicationId\n",
        "sc.sparkUser()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7hnhjRC9p6SC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#from disk\n",
        "sc.textFile('spark_data/spark_commitors.tsv').take(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sh61ayBTNXy8",
        "colab_type": "code",
        "outputId": "a8ea7d14-e9af-43b6-c80a-a49d709a8f93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#from memory\n",
        "sc.parallelize(['1','2','3'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "l81Qqg1MrFvZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!aws configure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OnS_gwv-ssF9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"AWS Access Key ID\")\n",
        "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"AWS Secret Access Key\")\n",
        "sc._jsc.hadoopConfiguration().set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hDNPO0XXLUYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Not able to check data directly from s3  (Make it work)\n",
        "sc.textFile(\"s3n://spark_data/spark_commitors.tsv\").take(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PCbUsynTNsZu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SparkSession**\n",
        "* Entry point to spark SQL\n",
        "* Merges SQLContext and HiveContext\n",
        "* Access SparkContext\n",
        "* Can have multiple SparkSession objects\n",
        "* Created for you in REPL\n",
        "* You need to create for spark2-submit\n"
      ]
    },
    {
      "metadata": {
        "id": "J_QaqfMLV1yT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spark\n",
        "type(spark)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gyUJ8i6ZOnKB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spark.sparkContext.textFile('spark_data/spark_commitors.tsv').take(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "07O05BmXQQxA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Convert to dataframe in Spark 2.0\n",
        "data = spark.read.option(\"header\",\"true\").option(\"delimiter\", \"\\t\").csv('spark_data/spark_commitors.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hEYkcV9tY3xP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creates a temporary view using the DataFrame\n",
        "data.createOrReplaceTempView(\"data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3mfhNGDZFxG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# SQL can be run over DataFrames that have been registered as a table.\n",
        "spark.sql(\"SELECT * FROM data\").show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sKjT8qZ6Mtky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mylist = [{\"activity_id\":1,\"activity_name\":\"xxx\"},{\"activity_id\":2,\"activity_name\":\"yyy\"},{\"activity_id\":3,\"activity_name\":\"zzz\"}]\n",
        "\n",
        "from pyspark.sql import Row\n",
        "\n",
        "spark.createDataFrame(Row(**x) for x in mylist).show(truncate=False)\n",
        "# I used **  (keyword argument unpacking) to pass the dictionaries to the Row constructor.\n",
        "# Keyword argument unpacking --> The *args and **kwargs is a common idiom to allow arbitrary number of arguments to functions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f3TBvINFE396",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Learning the core of Spark : RDDs\n"
      ]
    },
    {
      "metadata": {
        "id": "gNSKAA9QfhCg",
        "colab_type": "code",
        "outputId": "780c939a-abb5-4359-e6f2-456427af08f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sc.uiWebUrl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://0dd0d7c5dcbc:4040'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "vmJ3LwiwDg5X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf,SparkContext\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n9X-CbMbJs4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sc.getConf().getAll()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuFWvcUWKiHi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spark.ui.enabled"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}